{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de Contenido con Python\n",
        "\n",
        "El análisis de contenido es una técnica de investigación utilizada para descifrar información cualitativa en forma de texto. A través de técnicas específicas, podemos convertir texto en datos cuantitativos para su análisis. Python, con sus bibliotecas como NLTK y TextBlob, facilita este proceso."
      ],
      "metadata": {
        "id": "Ujw5VIAW85pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenización con NLTK\n",
        "\n",
        "La tokenización es el proceso de dividir el texto en palabras individuales o tokens. Es el primer paso esencial para cualquier análisis de texto."
      ],
      "metadata": {
        "id": "4RVy-V7J9UN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación y configuración inicial\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pRF60qO9fQE",
        "outputId": "889aa505-b3f2-4090-d666-eff5bbb28e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewzk9LCg8zkI",
        "outputId": "d9b7d760-8b4b-4484-bab6-4aebcbd4ecf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¡Hola', ',', 'mundo', 'del', 'análisis', 'de', 'datos', '!']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texto = \"¡Hola, mundo del análisis de datos!\"\n",
        "tokens = word_tokenize(texto, language='spanish')\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Eliminación de Stop Words\n",
        "\n",
        "Las stop words son palabras comunes que generalmente no aportan significado al análisis, como \"y\", \"la\", \"el\" en español. Eliminarlas ayuda a concentrarse en las palabras importantes."
      ],
      "metadata": {
        "id": "zDAee2AU9l4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "palabras_filtradas = [word for word in tokens if word.lower() not in stopwords.words('spanish')]\n",
        "print(palabras_filtradas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5oAEVog9lOb",
        "outputId": "17ba529d-da66-4052-9b7a-2e7b6c1b8e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¡Hola', ',', 'mundo', 'análisis', 'datos', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Análisis de Frecuencia\n",
        "\n",
        "El análisis de frecuencia nos permite identificar las palabras más comunes en un conjunto de datos, lo que puede ofrecer insights sobre el tema principal del contenido."
      ],
      "metadata": {
        "id": "-EBHKn7_9x8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "frecuencia = FreqDist(palabras_filtradas)\n",
        "print(frecuencia.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9Paf0df9vgn",
        "outputId": "07bf452d-1075-444b-bdac-8780c22e0952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('¡Hola', 1), (',', 1), ('mundo', 1), ('análisis', 1), ('datos', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Análisis de Sentimientos con TextBlob\n",
        "\n",
        "El análisis de sentimientos se refiere a la identificación y extracción de opiniones dentro de un texto. Puede ayudarnos a determinar el tono general del contenido."
      ],
      "metadata": {
        "id": "vc-qYQfT95QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(\"Este análisis es increíble!\")\n",
        "sentimiento = blob.sentiment.polarity\n",
        "print(\"Positivo\" if sentimiento > 0 else \"Negativo\" if sentimiento < 0 else \"Neutral\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhsGHS4l92Um",
        "outputId": "ffd005a7-88d0-4922-86ba-bd43c3fc6125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo: Análisis de Sentimientos en Reseñas de Películas\n",
        "\n",
        "Las reseñas de películas son una buena fuente de opiniones y sentimientos. Al analizar estas reseñas, podemos identificar tendencias en la percepción del público sobre ciertas películas o géneros. Este mismo análisis lo puedes aplicar a reseñas de tu e-commerce, comentarios de tus seguidores, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "DJ8_IK7Q-zhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación y configuración inicial\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Obteniendo reseñas de películas\n",
        "documentos = [(list(movie_reviews.words(fileid)), category)\n",
        "              for category in movie_reviews.categories()\n",
        "              for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Tomamos solo las primeras 100 reseñas positivas y 100 negativas para el análisis\n",
        "documentos_positivos = documentos[:100]\n",
        "documentos_negativos = documentos[1000:1100]\n",
        "\n",
        "# Procesamiento: Tokenización, eliminación de stop words y análisis de frecuencia\n",
        "tokens_positivos = [word for doc in documentos_positivos for word in doc[0] if word.lower() not in stopwords.words('english')]\n",
        "tokens_negativos = [word for doc in documentos_negativos for word in doc[0] if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "frecuencia_positivos = FreqDist(tokens_positivos)\n",
        "frecuencia_negativos = FreqDist(tokens_negativos)\n",
        "\n",
        "print(\"Palabras más comunes en reseñas positivas:\", frecuencia_positivos.most_common(5))\n",
        "print(\"Palabras más comunes en reseñas negativas:\", frecuencia_negativos.most_common(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZKDiLPO9-j3",
        "outputId": "05581ec6-e8de-434b-945c-8d81b1847493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras más comunes en reseñas positivas: [(',', 3485), ('.', 3139), (\"'\", 1602), ('\"', 884), ('-', 735)]\n",
            "Palabras más comunes en reseñas negativas: [(',', 4041), ('.', 3220), (\"'\", 1425), ('\"', 791), ('-', 742)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este ejemplo procesa reseñas de películas del corpus movie_reviews de NLTK, identificando las palabras más comunes en reseñas positivas y negativas. Es una forma sencilla de entender qué palabras o temas son recurrentes en opiniones positivas o negativas."
      ],
      "metadata": {
        "id": "ugHvxRCD_T-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota:\n",
        "\n",
        "Los términos \"corpus\" y \"dataset\" en algunos contextos se utilizan de manera intercambiable. Sin embargo, hay matices sutiles en su significado:\n",
        "\n",
        "+ **Corpus**: Se refiere específicamente a una colección de textos. Estos textos pueden ser de cualquier tipo: libros, artículos, transcripciones, etc. Un corpus es a menudo utilizado en lingüística y procesamiento del lenguaje natural (NLP) para estudiar patrones en el lenguaje.\n",
        "\n",
        "+ **Dataset**: Es un término más general que se refiere a cualquier conjunto de datos, ya sea que contenga texto, números, imágenes, etc. Un dataset puede ser utilizado en cualquier campo que requiera análisis de datos, desde la ciencia de datos hasta la investigación académica y más allá.\n",
        "\n",
        "Todo corpus es un dataset, pero no todo dataset es un corpus."
      ],
      "metadata": {
        "id": "Z7xDPJIE_hen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusión\n",
        "¡Gracias por acompañarme en este recorrido por el análisis de contenido con Python! Espero que estos ejemplos y técnicas te sean útiles en tus futuros proyectos. El mundo del procesamiento del lenguaje natural y el análisis de datos es muy diverso y siempre hay algo nuevo que aprender.\n"
      ],
      "metadata": {
        "id": "QAfe1iRpAHFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Material desarrollado con base en la presentación: Data Analysis for Content Creators: Boost Your Audience & Improve Content\n",
        "\n",
        "Por: Fernanda Ochoa.\n",
        "\n",
        "Redes:\n",
        "* GitHub: [FernandaOchoa](https://github.com/FernandaOchoa)\n",
        "* Twitter: [@imonsh](https://twitter.com/imonsh)\n",
        "* Instagram: [fherz8a](https://www.instagram.com/fherz8a/)\n"
      ],
      "metadata": {
        "id": "l0gdUvssAnaY"
      }
    }
  ]
}